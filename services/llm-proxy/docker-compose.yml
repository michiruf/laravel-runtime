services:
  llm-proxy:
    container_name: llm-proxy
    image: ghcr.io/berriai/litellm:main-stable
    ports:
      - '4000:4000'
    env_file:
      - path: '../../.env'
        required: true
    volumes:
      - './config.yaml:/app/config.yaml'
    command: ["--port", "4000", "--config=/app/config.yaml"]

services:
  llm-proxy:
    container_name: llm-proxy
    image: ghcr.io/berriai/litellm:main-stable
    ports:
      - '4000:4000'
    environment:
      - OPENAI_API_KEY
    volumes:
      - './config.yaml:/app/config.yaml'
    command: ["--port", "4000", "--config=/app/config.yaml"]
